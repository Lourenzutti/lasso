{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6050eb56-ab4c-4d68-ac6d-fc8ced6c162b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Applying LASSO: Computational Packages and Some Practical Issues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96c0ac9-5be6-4996-9b83-882231269c32",
   "metadata": {},
   "source": [
    "#### Rodolfo Lourenzutti\n",
    "#### 2023-05-30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8c40bf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture Plan and Objectives\n",
    "\n",
    "In this part 2 of the lecture, we will:\n",
    "- review the video material;\n",
    "- learn how to fit a LASSO model using `glmnet` and `cv.glmnet`;\n",
    "    - we will code this together! \n",
    "- discuss some caveats of LASSO in inference and categorical variables;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb937ee7-f673-4da1-87b2-883ed9f958db",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Review Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227d2d2c-7caf-4cab-9448-de6031081004",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**True or False:**\n",
    "\n",
    "We must standardize the predictors before applying LASSO. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552fcaa9-f931-4ae7-aa3b-da4876c47427",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Why? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889cab2a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$\n",
    "LASSO(\\beta) = \\sum_{i=1}^n \\left(y_i - \\beta_0 - \\beta_1X_{i1} - \\ldots - \\beta_p X_{ip}\\right)^2 + {\\color{red}{\\lambda\\sum_{i=1}^p \\left|\\beta_i\\right|}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521f92c5-e465-4a91-993c-bda8d04566c7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**True or False:**\n",
    "\n",
    "The penalty term compromises LASSO's ability to fit the intercept term properly. For this reason, we should always center the response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17305496-86ac-4036-9c7a-67fa5c429021",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**True or False:**\n",
    "\n",
    "When we increase the $\\lambda$ value of the penalty term, the absolute value of each coefficient will be reduced by LASSO. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4da2d5b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Imagine you have a restriction that $|\\beta_1|$ and $|\\beta_2|$ must be less than 1. \n",
    "  - You could have $|\\beta_1| = 0.6$ and $|\\beta_2| = 0.4$.\n",
    "  \n",
    "- Now, if we reduce our budget to 0.7. Possible results are:\n",
    "   - $|\\beta_1| = 0.7$ and $|\\beta_2| = 0$.\n",
    "   - $|\\beta_1| = 0$ and $|\\beta_2| = 0.7$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c66e98",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- The main idea is that the set of variables selected in a model with 4 variables might have different variables than in a model with 3 variables, or with 2 variables, and so on. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72471cd-7252-42fe-8460-ac02ad2212ae",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**True or False:**\n",
    "\n",
    "When we increase the $\\lambda$ value of the penalty term, the sum of the absolute values of the coefficients will be reduced by LASSO. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef11eabb-0b63-4bf4-a2a5-bb3d555f8a48",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**True or False:**\n",
    "\n",
    "LASSO's solution improves the Residual Sum of Squares in comparison to OLS, which is one of the reasons LASSO is superior to OLS. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9d3624",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Loading Packages\n",
    "\n",
    "library(tidyverse)\n",
    "library(gridExtra)\n",
    "library(tidymodels)\n",
    "library(broom)\n",
    "library(leaps)\n",
    "library(Brq)\n",
    "library(glmnet)\n",
    "library(knitr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b864ec24-c7a6-497a-977b-19fd4d6e8af0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Review the LASSO method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d02673-b50c-47bc-bdb1-2f5b688142a8",
   "metadata": {},
   "source": [
    "In LASSO, change the OLS objective function by adding a penalty term to it:\n",
    "\n",
    "$$\n",
    "LASSO(\\beta) = \\sum_{i=1}^n \\left(y_i - \\beta_0 - \\beta_1X_{i1} - \\ldots - \\beta_p X_{ip}\\right)^2 + {\\color{red}{\\lambda\\sum_{i=1}^p \\left|\\beta_i\\right|}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a22354",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- LASSO regularizes **and** selects features simultaneously;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2346189f-3568-4b58-935a-b3dc733be49d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The penalty term keeps the coefficients in check;\n",
    "    1. Large values of $\\lambda$ will...?\n",
    "    2. Small values of $\\lambda$ will...?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1433a0b2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- LASSO has no closed-form solution. We need to use algorithms; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd850f0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "options(repr.plot.width = 20, repr.plot.height = 10)\n",
    "\n",
    "\n",
    "lasso_large_lambda <- ggplot(tibble(x1 = -0.5,  x2 = 0, x3 = 0.5, x4 = 0,\n",
    "              y1 = 0, y2 = 0.5, y3 = 0, y4 = -0.5)) + \n",
    "  geom_segment(aes(x = x1, y = y1, xend = x2, yend = y2)) +\n",
    "  geom_segment(aes(x = x2, y = y2, xend = x3, yend = y3)) +\n",
    "  geom_segment(aes(x = x3, y = y3, xend = x4, yend = y4)) +\n",
    "  geom_segment(aes(x = x1, y = y1, xend = x4, yend = y4)) +\n",
    "  labs(x = 'β1', y = 'β2', title = 'Large Lambda') +\n",
    "  xlim(-1, 1) +\n",
    "  ylim(-1, 1) +\n",
    "  geom_point(aes(x,y), color = \"darkgreen\", data = tibble(x=c(0.78), y=c(0.25)), size = 5) + \n",
    "  annotate(\"text\", x = 0.78-0.025, y = 0.18, label = \"OLS solution\", size = 8) + \n",
    "  theme(text = element_text(size = 30))\n",
    "\n",
    "\n",
    "lasso_small_lambda <- ggplot(tibble(x1 = -2,  x2 = 0, x3 = 2, x4 = 0,\n",
    "              y1 = 0, y2 = 2, y3 = 0, y4 = -2)) + \n",
    "  geom_segment(aes(x = x1, y = y1, xend = x2, yend = y2)) +\n",
    "  geom_segment(aes(x = x2, y = y2, xend = x3, yend = y3)) +\n",
    "  geom_segment(aes(x = x3, y = y3, xend = x4, yend = y4)) +\n",
    "  geom_segment(aes(x = x1, y = y1, xend = x4, yend = y4)) +\n",
    "  labs(x = 'β1', y = 'β2', title = 'Small Lambda') +\n",
    "  xlim(-3, 3) +\n",
    "  ylim(-3, 3) +\n",
    "  geom_point(aes(x,y), color = \"darkgreen\", data = tibble(x=c(0.78), y=c(0.25)), size = 5) + \n",
    "  annotate(\"text\", x = 0.78-0.02, y = 0.025 + 0.06, label = \"OLS solution\", size = 8) + \n",
    "  theme(text = element_text(size = 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa81f4b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "grid.arrange(lasso_large_lambda, lasso_small_lambda, ncol=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b003daca",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fitting LASSO \n",
    "\n",
    "### Prostate Cancer Data Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed4217d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Do you want to learn how to fit LASSO to a dataset? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc993865",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](https://media4.giphy.com/media/v1.Y2lkPTc5MGI3NjExNDYxMzliMDc1NmRlMzVjMGFlZWVkZGIyZDI0MDA3MzRlZGU5M2M4NCZlcD12MV9pbnRlcm5hbF9naWZzX2dpZklkJmN0PWc/fypu26wRfQZVgu0Prx/giphy.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a9168e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Let's start by loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ca1e83",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "data(\"Prostate\")\n",
    "\n",
    "cancer_prostate <- \n",
    "    tibble(Prostate) \n",
    "\n",
    "# Printing the first 10 rows\n",
    "head(cancer_prostate, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f396473b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Data Dictionary\n",
    "- **lcavol**: log(cancer volume) \n",
    "- **lweight**: log(prostate weight)\n",
    "- **age**: age of the patient\n",
    "- **lbph**: log(amount of benign prostatic hyperplasia)\n",
    "- **svi**: Seminal vesicle invasion. (True or False)\n",
    "- **lcp**: log(capsular penetration)\n",
    "- **gleason**: Gleason score. The Gleason score measures how abnormal the tissue looks. The lower the score is, the more the cells look like regular prostate tissue. You can learn more about the [Gleason Score here](https://www.pcf.org/about-prostate-cancer/diagnosis-staging-prostate-cancer/gleason-score-isup-grade/)\n",
    "- **pgg45**: Percentage Gleason scores 4 or 5. Scores 4 and 5 are considered very high; the cells barely look like normal prostate tissue.\n",
    "- **lpsa**: log(prostate-specific antigen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06064626",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Unfortunately, the units for the variables are not readily available and are unknown. \n",
    "\n",
    "- However, remember that the units are fundamental to interpreting the model. \n",
    "\n",
    "- So always make sure you know what the units are (even though I'm not sure what the units are in this case &#x1F609;). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b77f3d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 1: Data splitting\n",
    "\n",
    "Even though we will be fitting the LASSO method and not comparing it with other models (such as., k-nn regression), there's still plenty we still need to learn. For example, we still need to figure out the following:\n",
    "\n",
    "1. which covariates are relevant? \n",
    "2. how much regularization should we use?\n",
    "\n",
    "We will use the data to make these decisions. But once we've selected the model, we need to be able to assess our model. We must use different data to do this. \n",
    "\n",
    "### Exercise 1\n",
    "\n",
    "So let's start by splitting the dataset into two sets: (1) train set, with 70% of the rows; and (2) test set, with the remaining 30%. \n",
    "\n",
    "_Save the train set in an object called `cancer_train` and test set in `cancer_test`._\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa0affe",
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(20230530) # Do not change this\n",
    "\n",
    "cancer_prostate_split <- initial_split(..., prop = ...)\n",
    "\n",
    "cancer_train <- training(...)\n",
    "cancer_test <- testing(...) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa48902",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercise 2 \n",
    "\n",
    "Let's take a look at some summary quantities of our data. First, remove the `svi` variable (we don't want to calculate the summary of a binary variable). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa8f6cf",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "... |> \n",
    "    ...(...) |>\n",
    "    summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70226e16",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Step 2: Data Standardization\n",
    "\n",
    "- <span style='color: red'>DO NOT FORGET:</span> we need to standardize the data before applying LASSO!\n",
    "\n",
    "- Luckily for us, `glmnet` automatically standardizes the data for us! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256eb98e",
   "metadata": {},
   "source": [
    "## Step 3: Calling  the `glmnet` function\n",
    "\n",
    "- `glmnet` is much more flexible than we need here. \n",
    "\n",
    "- So, let's focus only on the arguments that matter to us;\n",
    "\n",
    "- You can learn more with [`glmnet`'s vignette](https://glmnet.stanford.edu/articles/glmnet.html) or calling help `?glmnet`\n",
    "\n",
    "\n",
    "\n",
    "### Exercise 3\n",
    "\n",
    "To train a regression using LASSO, we can use the package `glmnet` in R. A few points to keep in mind:\n",
    "\n",
    "- `glmnet` actually uses a penalty function that generalizes LASSO. So, to reduce that generalization to LASSO, we set alpha = 1, which is the default. \n",
    "\n",
    "\n",
    "- we can define a grid of values to evaluate by providing `lambda`, or we can just say how many lambdas we want to try out, and the function will define the values on its own. \n",
    "\n",
    "Now, fill in the code below to run LASSO with our `cancer_train` dataset. We want to try out 200 values for `lambda`. Store the model in an object named `lasso_cancer_fitted`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34005f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(20230530)\n",
    "\n",
    "# Fill in the code\n",
    "\n",
    "lasso_cancer_fitted <- # The variable to store the fitted model\n",
    "    glmnet(\n",
    "        x = ... |> select(-...),  # First argument is the covariates (note that we need at least two columns)\n",
    "        y = cancer_train$...,  # Second argument is the response\n",
    "        alpha = 1, # This is the default and is equivalent to LASSO \n",
    "        nlambda = ..., # the number of lambda values to try (default is 100)\n",
    "        #lambda  = ..., # alternatively, the values of lambda you want try\n",
    "        standardize = TRUE, # Default value is TRUE\n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a11a12",
   "metadata": {},
   "source": [
    "## Step 4: Extracting information from fitted model\n",
    "\n",
    "- The fitted model is a complex R object. \n",
    "\n",
    "- We will use some functions to extract the main parts of our model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60b73c5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercise 4: the solution Path\n",
    "\n",
    "If we call the `plot` function directly on the fitted model and it will show the value of the coefficients for different values of lambda. We need to specify `xvar = 'lambda'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ca7c2f",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "...(..., xvar = 'lambda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34da8f6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercise 5: grabbing the coefficients\n",
    "\n",
    "To see what the coefficients are, we call the `coef` function. Naturally, the values of coefficients depend on lambda. We can specify one or more values of lambda using the `s` argument. \n",
    "- If you don't specify an `s` value, coefficients for all fitted $\\lambda$ will be returned;\n",
    "- If you pick a $\\lambda$ not fitted by the model, it returns an \"approximation\" of the coefficients via interpolation; usually, this approximation is quite accurate. \n",
    "\n",
    "Obtain the coefficients for the model with $\\lambda=0.4$ and $\\lambda=0.1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f020689b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "coef(..., s = c(...)) # here lambda is called `s` (as in \"size\" of the model, but lower s --> larger models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c29cf8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example 6: \n",
    "\n",
    "You can specify `exact = TRUE` to fit the model for a given $\\lambda$ and obtain the exact coefficients.\n",
    "  - In this case, we need to provide $x$ and $y$ again;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3282ba8b",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "coef(lasso_cancer_fitted, \n",
    "     s = 0.4, \n",
    "     exact = TRUE, \n",
    "     x = cancer_train |> select(-lpsa),\n",
    "     y = cancer_train$lpsa)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9d1882",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 5: making predictions\n",
    "\n",
    "### Exercise 6\n",
    "\n",
    "For prediction, the flow is very similar. We call the `predict` function and specify one or more lambda values we want in the `s` argument, but we also need to provide the `newx` matrix containing the data to be used for prediction. \n",
    "\n",
    "Unfortunately, `glmnet` doesn't play well with data frames and requires a matrix for `newx`.\n",
    "\n",
    "Use `cancer_train` data to obtain the predictions for the model `lasso_cancer_fitted` using  $\\lambda=0.4$ and $\\lambda=0.1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28de91ff",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "     \n",
    "predict(lasso_cancer_fitted, # Fitted model\n",
    "    newx = ... |> select(-...) |> as.matrix(), # A matrix (yes, tibble don't work) containing the data for prediction,\n",
    "    s = c(..., ...) # one or more values of lambda\n",
    "   ) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aff4e3d-2e40-4e65-8935-8f63c8b62a24",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Tuning: the process of selecting  $\\lambda$: \n",
    "\n",
    "- We saw that `glmnet` fits LASSO for many different values of $\\lambda$. Which $\\lambda$ should we use? \n",
    "\n",
    "- This is called hyper-parameter tuning, and we usually do this using cross-validation. \n",
    "\n",
    "- We decide on a metric, such as MSE, and pick $\\lambda$ with the best performance in the cross-validation set. \n",
    "\n",
    "### Exercise 7\n",
    "\n",
    "Luckily for us, we don't need to do this ourselves. Instead, we can call `cv.glmnet`, which will do this entire process for us. The function is very similar to `glmnet`. Fill in the code below to run `cv.glmnet`. \n",
    "\n",
    "_Save the fitted model in an object called `cancer_cv_lasso`._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7286570a",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "cancer_cv_lasso <- \n",
    "    cv.glmnet(\n",
    "        x = ... |> ... |> as.matrix(), # Covariates\n",
    "        y = ..., # Response\n",
    "        lambda = NULL, # A sequence of lambdas. Default lets the function choose its own sequence. \n",
    "        nfolds = 10, # number of folds to be used in the cross validation\n",
    "        standardize = TRUE\n",
    "    )\n",
    "\n",
    "print(cancer_cv_lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a0dde2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## `plot` functions for `cv.glmnet`\n",
    "\n",
    "The `coef` and `predict` functions work exactly the same as for the `glmnet`. But the plot function is a little different. Let's check! \n",
    "\n",
    "### Exercise 8\n",
    "\n",
    "Call the `plot` function directly on the fitted `cv.glmnet` model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde6b53d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plot(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d092e5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The plot shows the estimated MSE for different value of lambdas; \n",
    "\n",
    "- The numbers at the top show how many variables are included in the model for each lambda; \n",
    "\n",
    "- The error bars represent the variation of MSE over the different folds of the cross-validation;\n",
    "\n",
    "- First vertical dotted line (on the left), the best CV $\\lambda$ value; \n",
    "\n",
    "- Second vertical dotted line (on the right), the most regularized model (highest $\\lambda$) that the CV performance is 1 std. dev. from the best one. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50faa166",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## `coef`, and `predict` functions for `cv.glmnet`\n",
    "\n",
    "- They work pretty much the same way;\n",
    "\n",
    "- Except now we can use two string for `s` argument:\n",
    "    - \"lambda.1se\"\n",
    "    - \"lambda.min\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a7143e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "coef(cancer_cv_lasso, \n",
    "    s = 'lambda.1se') # two new possible values:  \"lambda.min\" or \"lambda.1se\" (default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81887bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "coef(cancer_cv_lasso, \n",
    "    s = 'lambda.min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66aed716",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "predict(\n",
    "    cancer_cv_lasso, \n",
    "    newx = cancer_train |> select(-lpsa) |> as.matrix(),\n",
    "    s = 'lambda.1se') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f1b3c2",
   "metadata": {},
   "source": [
    "# A couple of \"caveats\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b312718e",
   "metadata": {},
   "source": [
    "## LASSO and categorical variable\n",
    "\n",
    "- When we fit categorical data, we use dummy variables (one-hot-encoding); \n",
    "\n",
    "\n",
    "- What happens if LASSO removes one of the dummy variables but leaves the others? Does it make sense? \n",
    "\n",
    "\n",
    "- There are some LASSO extensions, such as Group LASSO, that deal with this. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1179d4fa-8601-41c7-b766-33087bb05c36",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LASSO Inference\n",
    "\n",
    "- The LASSO method gives biased estimates for the parameters; \n",
    "    - Biased estimates are not the end of the world, but can make inference trickier; \n",
    "    \n",
    "- Confidence intervals are not immediately available for LASSO\n",
    "   - There are a few proposals for confidence intervals available for coefficients and hypothesis testing for LASSO models; "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead2e9eb",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The trickier part is that inference for LASSO triggers the post-selection inference problem;\n",
    "  - I have prepared an activity to walk you through this problem and illustrate the bias in the LASSO estimate. \n",
    "  \n",
    "  \n",
    "- An approach to deal with this is to use data split. \n",
    "    - In the first split, we run LASSO to select the variables; \n",
    "    - In the second split, we fit OLS and use that inference; "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.2.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
